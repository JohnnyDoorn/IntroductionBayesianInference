[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Brief Introduction to Bayesian Inference",
    "section": "",
    "text": "Preface\nThis book is still a work in progress\nIf you encounter any errors/issues, you can reach me here.\nThis booklet offers an introduction to Bayesian inference. We look at how different models make different claims about a parameter, how they learn from observed data, and how we can compare these models to each other. We illustrate these ideas through an informal beer-tasting experiment conducted at the University of Amsterdam.1 A key concept in Bayesian inference is predictive quality: how well did a model, or parameter value, predict the observed data? We use this predictive quality to update our knowledge about the world, and then use the updated knowledge to make predictions about tomorrow’s world. This learning cycle is visualized below, and will be revisited throughout the booklet.\nIn the first chapters, the basic Bayesian ingredients (models, prior, posterior, Bayes factor) will be disucssed. In the chapters that follow, these ingredients are used to cook up results for the beer-tasting experiment. Specifically, the Bayesian binomial test, correlation test, and \\(t\\)-test will be demonstrated.\nBayesian learning cycle."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Brief Introduction to Bayesian Inference",
    "section": "",
    "text": "In fact, this text is an elaborated version of an article we published on the experiment, see van Doorn, Matzke, and Wagenmakers (2020)↩︎"
  },
  {
    "objectID": "01-intro.html#a-bayesian-version",
    "href": "01-intro.html#a-bayesian-version",
    "title": "1  The Lady Tasting Tea",
    "section": "1.1 A Bayesian Version",
    "text": "1.1 A Bayesian Version\nDecades later, Dennis Lindley (1993) used an experimental procedure similar to that of Fisher to highlight some limitations of the \\(p\\)-value paradigm. Specifically, the calculation of the \\(p\\)-value depends on the sampling plan, that is, the with which the data were collected. Consider the Lindley setup: Dr. Bristol is offered six pairs of cups, where each pair consists of a cup where the tea was poured first, and a cup where the milk was poured first. She is then asked to judge, for each pair, which cup has had the tea added first. A possible outcome is the sequence RRRRRW, indicating that she was right for the first five pairs, and wrong for the last pair. However, as Lindley demonstrated, the original sampling plan is crucial in calculating the \\(p\\)-value because the \\(p\\)-value depends on hypothetical outcomes that are “more extreme.”\nWas the goal to have the Dr. Bristol taste six pairs of cups –no more, no less– or did she need to continue until she made her first mistake? The observed data could have been the outcome of either sampling plan; yet in the former case, the \\(p\\)-value equals \\(0.109\\), whereas in the latter case the \\(p\\)-value equals \\(0.031\\). The difference lies in the inclusion of more extreme cases. In the “test six cups” plan, the only more extreme outcome is RRRRRR (i.e., she correctly identified all 6 cups), whereas for the “test until error” plan the more extreme outcomes include sequences such as RRRRRRW and RRRRRRRW (i.e., 6 and 7 correct responses, before a single incorrect response)1. It seems undesirable that the \\(p\\)-value depends on hypothetical outcomes that are in turn determined by the sampling plan. Harold Jeffreys summarized: ``What the use of \\(p\\) implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred. This seems a remarkable procedure.” (Jeffreys 1961, p385).\nThis drawback is one of the reasons (for more critique see (Wasserstein and Lazar 2016; Benjamin et al. 2018)) why Bayesian inference has become more popular in the past years as an alternative framework for hypothesis testing and parameter estimation."
  },
  {
    "objectID": "01-intro.html#an-alcoholic-version",
    "href": "01-intro.html#an-alcoholic-version",
    "title": "1  The Lady Tasting Tea",
    "section": "1.2 An Alcoholic Version",
    "text": "1.2 An Alcoholic Version\nIn this text we revisit Fisher’s experimental paradigm to demonstrate several key concepts of Bayesian inference, specifically the prior distribution, the posterior distribution, the Bayes factor, and sequential analysis. Furthermore, we highlight the advantages of Bayesian inference, such as its straightforward interpretation, the ability to monitor the result in real-time, and the irrelevance of the sampling plan. For concreteness, we analyze the outcome of a tasting experiment that featured 57 staff members and students of the Psychology Department at the University of Amsterdam; these participants were asked to distinguish between the alcoholic and non-alcoholic version of the Weihenstephaner Hefeweissbier, a German wheat beer. We analyze and present the results in the open-source statistical software JASP (JASP Team 2022).\n\n\n\n\nBenjamin, D. J., J. O. Berger, M. Johannesson, B. A. Nosek, E.–J. Wagenmakers, R. Berk, K. A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2: 6–10.\n\n\nFisher, Ronald Aylmer. 1937. The Design of Experiments. Oliver And Boyd; Edinburgh; London.\n\n\nJASP Team. 2022. “JASP (Version 0.16.3)[Computer software].” https://jasp-stats.org/.\n\n\nJeffreys, H. 1961. Theory of Probability. 3rd ed. Oxford, UK: Oxford University Press.\n\n\nLindley, D. V. 1993. “The Analysis of Experimental Data: The Appreciation of Tea and Wine.” Teaching Statistics 15: 22–25.\n\n\nWasserstein, R. L., and N. A. Lazar. 2016. “The ASA’s Statement on p–Values: Context, Process, and Purpose.” The American Statistician 70: 129–33."
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  The Lady Tasting Tea",
    "section": "",
    "text": "A more technical way of describing the difference between the two setups is that data from the “test six cups” follow the binomial distribution, whereas data from the “test until error” follow the negative binomial distribution.↩︎"
  },
  {
    "objectID": "02-models.html#sec-models-make-predictions",
    "href": "02-models.html#sec-models-make-predictions",
    "title": "2  What is a Model?",
    "section": "2.1 Models Make Predictions",
    "text": "2.1 Models Make Predictions\nAn essential property of a statistical model is that it can make predictions about the real world. We can use the accuracy of these predictions to gauge the quality/plausibility of a model, relative to another model. For instance, Sarah thinks that the probability of heads in a coin flip is 50% (i.e., \\(H_S: \\theta = 0.5\\)), while Paul claims that the coin has been tampered with, and that the probability of heads is 80% (i.e., \\(H_P: \\theta = 0.8\\)). Here, Sarah and Paul postulate different models/hypotheses. They are both binomial models, which is the general statistical model for describing a series of chance-based events with a binary outcome (e.g., coin flip, red/black in roulette, whether a random person from the population has a certain disease or not, or someone identifying the alcholic beer). Where Sarah and Paul differ, however, is their claim about the specific value of the \\(\\theta\\) parameter. In the remainder of this text, we will be referring to model to mean such a combination of general statistical model, and claim about the value of the model parameter (i.e., hypothesis).\n\n\n\n\n\nFigure 2.1: Two models for a coin toss. The arrows indicate what each of the models postulate: both postulate a single value for theta.\n\n\n\n\nThe two models make a different claim about \\(\\theta\\), and therefore also make different predictions about the outcome of a series of 10 coin flips. Specifically, we can use the binomial model to calculate how likely each possible outcome is under each of the models. For instance, we can calculate how likely it is to observe 8 heads out of 10 flips. The binomial formula is as follows: \\[\\begin{align}\n\\label{binomFormula}\nP(\\text{data} \\mid \\theta) = \\frac{n!}{k! (n-k)!} \\theta^k\\left(1-\\theta\\right)^{n-k},\n\\end{align}\\] which, if we fill in the outcome for which we want to know the likelihood (i.e., \\(k=8\\) heads out of \\(n=10\\) flips), becomes: \\[\\begin{equation}\nP(\\text{8 heads out of 10} \\mid \\theta) = \\frac{10!}{8! (10-8)!} \\theta^8\\left(1-\\theta\\right){10-8}.\n\\end{equation}\\] The last element to fill in is \\(\\theta\\). If we do so for Sarah, who postulates \\(\\theta = 0.5\\), we get 0.0439. For Paul, who postulates \\(\\theta = 0.8\\), we get 0.302. If we do this for every possible outcome, and create a bar graph of each likelihood, we get the following two figures that illustrate what each model deems likely (the yellow bar indicates each models’ likelihood of the example of 8 heads out of 10 flips):\n\n\n\n\n\nFigure 2.2: The likelihoods of all possible outcomes of 10 coin flips, under Sarahs model and under Pauls model. The yellow bar indicates the likelihood of the observed data (8 heads).\n\n\n\n\n\nThese two figures reflect likely outcomes of the experiment of flipping a coin 10 times. If Sarah is correct, and the probability of heads is in fact \\(0.5\\), likely outcomes are 4, 5, and 6. However, if Paul is correct, and \\(\\theta = 0.8\\), it is more likely to see 7, 8 or 9 heads."
  },
  {
    "objectID": "02-models.html#sec-model-comparison",
    "href": "02-models.html#sec-model-comparison",
    "title": "2  What is a Model?",
    "section": "2.2 Model Comparison",
    "text": "2.2 Model Comparison\nIn the previous section we have made concrete what each of the two models predict. The models differ in their statement about \\(\\theta\\) (Figure 2.1), and therefore differ in what they deem likely outcomes (Figure 2.2). Now imagine that we actually gather some data by flipping a coin 10 times, and we observe 8 heads and 2 tails. Figure 2.2 tells us that the probability of that happening under Sarahs model is 0.0439, while under Pauls model this is 0.302. These two numbers tell us something about how well each model predicted the data, relative to each other. Specifically, the ratio of these two numbers is known as the Bayes factor. Here, the Bayes factor is equal to 0.15, which means that the observed data are about 0.15 times more likely under Sarahs model than under Pauls model. The Bayes factor has a subscript that indicates what model is being compared to what: \\(\\text{BF}_{SP}\\) gives how much more likely Sarahs model is than Pauls, while \\(\\text{BF}_{PS}\\) gives how much more likely Pauls is than Sarahs. To go from \\(\\text{BF}_{SP}\\) to \\(\\text{BF}_{PS}\\), you simply take 1 divided by the other: \\(\\text{BF}_{PS}\\) = \\(\\frac{1}{\\text{BF}_{SP}}\\) = 6.872. So saying that the data are 6.872 times more likely under Pauls model than Sarahs model is exactly the same. Generally, it is a bit easier to communicate the Bayes factor that is \\(&gt;1\\), using the appropriate subscript.\nLastly, it can be the case that two models predicted the data equally well. In this case the Bayes factor will be equal to 1. Generally, we want the Bayes factor to be as far away from 1 as possible, since this indicates more and more evidence in favor of one model over another. Different categorizations have been made to translate a Bayes factor into human words, to facilitate communication about degrees of evidence for/against one model respective to another. One such representation is given below in Figure 2.3.\n\n\n\n\n\nFigure 2.3: A graphical representation of a Bayes factor classification table. As the Bayes factor deviates from 1, which indicates equal support for \\(H_0\\) and \\(H_1\\), more support is gained for either \\(H_0\\) or \\(H_1\\). The probability wheels illustrate the continuous scale of evidence that Bayes factors represent. These classifications are heuristic and should not be misused as an absolute rule for binary all-or-nothing conclusions."
  },
  {
    "objectID": "02-models.html#sec-more-models",
    "href": "02-models.html#sec-more-models",
    "title": "2  What is a Model?",
    "section": "2.3 More Models",
    "text": "2.3 More Models\nSo far, we have considered two models (\\(H_S\\) and \\(H_P\\)), both of which postulate a single value for the model parameter \\(\\theta\\). However, it is possible for a model to be more uncertain in its assertions. For instance, we can have a model that postulates that the probability of heads is greater than \\(0.5\\) (i.e., \\(0.5 \\leq \\theta \\leq 1\\) 1). This corresponds to the belief that the coin is tampered with, but without making a strong statement about the degree of tampering. Furthermore, next to the model postulating the range for \\(\\theta\\), it also needs to specify how likely it deems every value in this range. Let’s add two more people and their models to the mix to illustrate. Betty believes the coin has been tampered with, but is unsure about the degree of tampering: she believes that every value of \\(\\theta\\) between 0.5 and 1 is equally likely. Next is David, who is a bit more extreme in his beliefs: he believes that the coin is tampered with heavily, so assumes that values of \\(\\theta\\) close to 1 are more likely than values of \\(\\theta\\) closer to \\(0.5\\). If we were to plot the models and corresponding hypotheses of Betty and David, they would look as follows (the difference in density reflecting their different beliefs):\n\n\n\n\n\nFigure 2.4: Two more models for a coin toss. The colored regions indicate what each model believes. Even though both Betty and David belive the probabilty of heads to be greater than 0.5, they differ in how plausible they deem specific values in that range.\n\n\n\n\nCompared to the models in Figure 2.1, which only “bet” on a single value, the models above spread their bets more. David and Betty thus make safer bets since they make wider predictions. Although both Betty and David only consider positive values, they differ in how plausible they deem specific positive values. As before, these models also make predictions about how likely various outcomes of a series of 10 coin flips would be. Again, the binomial formula can be used. However, this time the models do not predict a single value, but a whole range of values. In order to compute how likely Betty’s model deems an outcome of 8 heads out of 10 flips, we have to consider every postulated value of \\(\\theta\\) between 0.5 and 1, compute the likelihood of the data for each value, and average across all of these likelihoods, weighted by the density at each point. The technical term for such weighted averaging is called marginalizing, and we refer to this averaged likelihood as the marginal likelihood. In the next section we will revisit this topic.\nIn Figure 2.5 below, you can see the marginal likelihoods for all outcomes, for each of the two additional models. Note that even though neither Betty nor David postulate values of \\(\\theta\\) below 0.5 (i.e., the parameter), they assign some plausibility to observed proportions below 0.5 (i.e., the statistic, or observed data). This reflects the random nature of a coin flip: even though the true probability of heads is \\(0.6\\), you might still observe 3 heads out of 10 flips.\n\n\n\n\n\nFigure 2.5: The marginal likelihoods of all possible outcomes of 10 coin flips, under the two additional models. The yellow bar indicates the marginal likelihood of the observed data (8 heads).\n\n\n\n\n\n2.3.1 The Open-Minded Model\nLastly, but perhaps most importantly, we can also consider a model that tries to spread its bets as much as possible. Let’s say that Alex wants to keep as much of an open mind about values of \\(\\theta\\) as possible. They consider each possible value of \\(\\theta\\) to be equally plausible. In Bayesian inference, we also refer to this type of model as the uninformed model. The figure below illustrates what the uninformed model posits, and which outcomes it deems likely. We again have a model that postulates multiple values, so the figure on the right depicts marginal likelihoods. For instance, for the yellow bar, we look at how likely 8 heads out of 10 flips are, averaged over all values postulated by the model, weighted by the density in the left graph.\n\n\n\n\n\nFigure 2.6: The so-called “uninformed model”. Alex wants to keep an open mind about the values of theta and considers each value equally plausible. Left: the colored region indicate what Alex believes. Right: what this specific model considers likely outcomes. The yellow bar indicates the marginal likelihood of the observed data (8 heads)."
  },
  {
    "objectID": "02-models.html#sec-more-model-comparison",
    "href": "02-models.html#sec-more-model-comparison",
    "title": "2  What is a Model?",
    "section": "2.4 More Model Comparisons",
    "text": "2.4 More Model Comparisons\nWe can apply the same principles from Section 2.2 to compare how well each of the additional models has predicted the observed data of 8 heads out of 10 coin flips. To do so, we can simply take the ratio of each of the yellow bars in the figures that depict how likely each model considers the various possible outcomes of 10 coin flips. For instance, Alex’ model has a marginal likelihood of 0.0909 for 8 heads, whereas Betty’s model has a marginal likelihood of 0.1745 for 8 heads. If we want to compare the predictions of Betty and Alex, we can look at the ratio of these values to obtain \\(\\text{BF}_{AB} =\\) 0.5, which is equivalent to \\(\\text{BF}_{BA} =\\) 1.9. This means that the data are about twice as likely under Betty’s model than under Alex’ model, which can be considered weak evidence in favor of Betty’s model over Alex’ model.\nIf we were to use the betting analogy again, we could say that while both Alex and Betty had bet some money on the outcome of 8 heads, Betty had bet more money on this particular outcome than Alex, and is therefore rewarded more. Because Betty has a more specific belief (namely that the coin is biased towards heads), she had more money at her disposal for betting on the considered values (i.e., values between 0.5 and 1). In contrast, Alex played it very safely: they win some money for any outcome because they spread their betting money across all values. However, because of this, their reward is lower for having correctly predicted the observed data compared to someone who made a more specific bet on the observed data. The phenomenon of more specific models being rewarded more (when predicting well) than their non-specific competitor is known as parsimony, and will be discussed in more depth in Chapter 4.\nA last model comparison we can make is to compare Alex’ model to Sarah’s model. In a typical (two-sided) statistical test about a proportion, this is the most often-used comparison: Sarah’s model is considered to be the null model, and Alex’ model is considered the two-sided alternative model. As we saw, Alex’ marginal likelihood is 0.0909, while Sarah’s marginal likelihood is 0.0439, so the Bayes factor comparing these two models, \\(\\text{BF}_{AS}\\), equals 2.07. This means the data are about twice as likely under Alex’ model compared to Sarah’s model.\nAs a bonus, when we know \\(\\text{BF}_{BA}\\) and \\(\\text{BF}_{AS}\\), we automatically know \\(\\text{BF}_{BS}\\). Since we know how much more likely the data are under Betty’s model than under Alex’ model (about 2 times), and how much more likely the data are under Alex’ model than under Sarah’s model (about 2 times), we also now know that Betty’s model is about \\(2 \\times 2 = 4\\) times more likely than Sarah’s model! This property it known as transitivity."
  },
  {
    "objectID": "02-models.html#concluding-thoughts",
    "href": "02-models.html#concluding-thoughts",
    "title": "2  What is a Model?",
    "section": "2.5 Concluding Thoughts",
    "text": "2.5 Concluding Thoughts\nIn this chapter, we have seen several key ideas:\n\nModels make concrete statements about parameters of a model. In this case, about the \\(\\theta\\) parameter in the binomial model\nThese statements can be characterized by a probability distribution, where the probability mass reflects the specific statement\n\nThe model could hypothesize a single value (e.g., the models of Sarah and Paul)\nThe model could hypothesize a range of values (e.g., the models of Betty, David and Alex)\n\nAfter we have observed some data, we can use the Bayes factor to compare the quality of the predictions made by each model\n\nThe Bayes factor is a relative metric, comparing 2 models at a time\nThe subscript of the Bayes factor indicates which model is compared to which\nMore specific predictions, when accurate, are rewarded (parsimony)\n\n\nInstead of comparing models, however, we can also look at one model, and use it to estimate the value of \\(\\theta\\). We will see that each of the models presented above will yield different estimates, because they had different a priori (i.e., before seeing the data) beliefs about plausible values of \\(\\theta\\)."
  },
  {
    "objectID": "02-models.html#footnotes",
    "href": "02-models.html#footnotes",
    "title": "2  What is a Model?",
    "section": "",
    "text": "Because we are working with a continuous range of values for \\(\\theta\\), the difference between saying \\(0.5 &lt; \\theta\\) and \\(0.5 \\leq \\theta\\) is infinitesimally small and the two versions may be used interchangeably.↩︎"
  },
  {
    "objectID": "03-estimation.html#models-have-beliefs",
    "href": "03-estimation.html#models-have-beliefs",
    "title": "3  How do Models Estimate?",
    "section": "3.1 Models Have Beliefs",
    "text": "3.1 Models Have Beliefs\nBelief is a funny word and we tend to associate the word with things that have seemingly little to do with science or statistics. However, we have beliefs about anything in the world – they might be quite weak, but we still have some starting point for reasoning about some phenomenon. For instance, I know very little about the number of penguins in the world, but I do know there are more than 0, probably more than \\(1{,}000\\), and fewer than \\(1{,}000{,}000{,}000\\) of them.1 I could characterize my very uninformed belief by means of a proability distribution, where the probabilty mass depicts how plausible I deem certain values. This is exactly the same as we did in the previous chapter by characterizing the different peoples’ beliefs about the fairness of a coin. In this case, even if we would know absolutely nothing about the coin in question, we still have some information about the parameter \\(\\theta\\). For instance, we know that it will be between 0 and 1 because it is a probability. If we do not have any knowledge beyond this, we could reflect our prior belief about \\(\\theta\\) by means of a uniform distribution, just like Alex did. In fact, we will now illustrate belief updating by looking at how Alex updates their beliefs, as a result of observing 8 heads out of 10 flips."
  },
  {
    "objectID": "03-estimation.html#updating-beliefs",
    "href": "03-estimation.html#updating-beliefs",
    "title": "3  How do Models Estimate?",
    "section": "3.2 Updating Beliefs",
    "text": "3.2 Updating Beliefs\nAs you might recall from Section 2.3.1, Alex was keeping an open mind about the values \\(\\theta\\) - their prior distribution2 was a uniform distribution across all values between 0 and 1.\n\n\n\n\n\nFigure 3.1: The so-called “uninformed model”. Alex wants to keep an open mind about the values of theta and considers each value equally plausible. The uniform prior distribution below reflects this.\n\n\n\n\nNow that we have formalized the prior beliefs about a parameter in the form of a probability distribution, we can start updating those beliefs with observed data. The belief updating factor consists of two parts.      \n\n3.2.1 The Likelihood\nThe first part of the updating factor, \\(p( \\text{data} \\mid \\theta)\\), expresses the likelihood of the observed data for all of the values postulated by the model. Here, this means all values between 0 and 1. For instance, we look at the likelihood of the observed data, given a \\(\\theta\\) value of 0.1. Just as before, we can use the binomial formula for this: \\[\\begin{align}\n\\label{eq-binom-formula-two}\n\\frac{n!}{k! (n-k)!} \\theta^k\\left(1-\\theta\\right)^{n-k},\n\\end{align}\\] which for \\(n = 10\\), \\(k = 8\\), and \\(\\theta = 0.1\\) gives 0. We can compute this value for all of the values between 0 and 1. If we do so, we can make the following graph that shows the likelihood of the data, for each value of \\(\\theta\\):\n\n\n\n\n\nFigure 3.2: The likelihood of observing 8 heads out of 10 flips, for all possible values of theta.\n\n\n\n\nAs you can see, the likelihood is the greatest for \\(\\theta = 0.8\\). This makes sense because the observed proportion is equal to \\(0.8\\). In short, this likelihood function depicts how well each possible value of \\(\\theta\\) predicted the observed data. In the previous section we saw two people who postulated only a single value for \\(\\theta\\). The likelihoods of their models can also be read from the above graph: Sarah (\\(P(\\text{data} \\mid \\theta = 0.5) =\\) 0.0439) and Paul (\\(P(\\text{data} \\mid \\theta = 0.8) =\\) 0.302).\nIt is important to note that the likelihood is not a probability distribution: its surface area does not sum to 1, and we therefore cannot use it to make probabilistic statements about the parameter (we can use the posterior distribution for this, at the end of this section).\n\n\n3.2.2 The Marginal Likelihood\nNow we can take a look at the second part of the updating factor: \\(p( \\text{data})\\). This part is known as the marginal likelihood. In contrast to the first part, the marginal likelihood is a single number. Namely, it is the average of all the likelihoods, where the likelihood of each value is weighted by the prior belief placed on that value by the model. This is the same procedure as in the previous chapter (see Section 2.3). In fact, for all of the models, the marginal likelihood was indicated by the yellow bar: it is the likelihood of the observed data, weighted by each model’s specific beliefs. In the case of Alex’ model, the prior belief is equal across all values, so the marginal likelihood is “simply” the average likelihood.3 In this case, the marginal likelihood is equal to 0.0909 - precisely the height of the yellow bar in Figure 2.6.\nWe can use this average likelihood to see which possible values of \\(\\theta\\) predicted the data better than average, and which values predicted the data worse than average: since the likelihood reflects the predictive quality of each value, the marginal likelihood reflects the average quality across all values. In the following figure, you can again see the likelihood function from Figure 3.2, but now with the marginal likelihood added:\n\n\n\n\n\nFigure 3.3: The likelihood of observing 8 heads out of 10 flips, for all possible values of theta. The purple line is the marginal likelihood, to visualize which values predicted the observed data better than average.\n\n\n\n\nBy doing so, we have quite literally set the bar: values of \\(\\theta\\) where the likelihood is greater than the marginal likelihood (approximately values between \\(0.55\\) and \\(0.95\\)) predicted the data better than average. In other words, \\(p( \\text{data} \\mid \\theta) &gt; p( \\text{data})\\). This means that the updating factor (i.e., the ratio of the likelihood and marginal likelihood) will be greater than 1. This in turn means that the posterior belief for those values will be greater than the prior belief. As a result of observing the data, values between \\(0.55\\) and \\(0.95\\) have an increase in plausibility. The reverse holds for values whose likelihood is lower than the marginal likelihood: those values have suffered a decrease in plausibility as a result of the data. Perhaps another look at the Bayesian updating formula makes more sense now than it did at first: \\[\\begin{align}\n\\underbrace{ p(\\theta \\mid \\text{data})}_{\\substack{\\text{Posterior beliefs}}} \\,\\,\\, = \\,\\,\\,\n\\underbrace{ p(\\theta)}_{\\substack{\\text{Prior beliefs} }}\n\\,\\,\\,\\, \\times\n\\overbrace{\\underbrace{\\frac{p( \\text{data} \\mid \\theta)}{p( \\text{data})}}}^{\\substack{\\text{Prediction for specific }\\theta }}_{\\substack{\\text{Average prediction} \\\\\\text{across all }  \\theta's}}.\n\\end{align}\\]\nWe can visualize the aforementioned process. Again we look at the likelihood values, but now the values that see an increase/decrease in plausibilty are marked by the blue/vermillion arrows, respectively:\n\n\n\n\n\nFigure 3.4: Values of theta that predicted the data better than average (marked in blue) will have a updating factor greater than 1, and receive a boost in plausibility as a result of the data. The reverse holds for values that predicted worse than average (marked in vermillion)."
  },
  {
    "objectID": "03-estimation.html#updated-beliefs",
    "href": "03-estimation.html#updated-beliefs",
    "title": "3  How do Models Estimate?",
    "section": "3.3 Updated Beliefs",
    "text": "3.3 Updated Beliefs\nWe started this section with the prior beliefs of Alex’ model, which we updated by looking at which values predicted the data well, in order to form posterior beliefs. Just as with prior beliefs, our posterior beliefs are in the form of a posterior distribution. The figure below shows both distributions, including the vertical lines from Figure 3.4 to illustrate the boost/penalty in plausibility.\n\n\n\n\n\nFigure 3.5: Prior and posterior distribution.\n\n\n\n\nWe have now updated Alex’ prior knowledge to posterior knowledge! The posterior distribution enables us to make probabilistic statements about values of \\(\\theta\\) because it is a probability distribution.4 For instance, the median of this posterior distribution is \\(0.764\\), which means that under Alex’ model, there is a 50% probability that \\(\\theta\\) is greater than \\(0.764\\). Or, we can use the posterior distribution to make an interval estimation. In the Bayesian framework, this is known as the credible interval. This interval entails taking the middle \\(x\\)% of the posterior distribution. For instance, we can take a 95% credible interval, which ranges from \\(0.482\\) to \\(0.940\\) in this case. This means that under Alex’ model, there is a 95% probability that the true value of \\(\\theta\\) is between \\(0.482\\) and \\(0.940\\). Such a straightfoward interpretation is one of the strengths of the Bayesian framework, compared to the frequentist framework."
  },
  {
    "objectID": "03-estimation.html#more-models",
    "href": "03-estimation.html#more-models",
    "title": "3  How do Models Estimate?",
    "section": "3.4 More Models",
    "text": "3.4 More Models\nUp until this point, we have only looked at Alex’ model: it had a specific starting point (i.e., prior belief), which was updated with the observed data to form an ending point. However, in the previous chapter we saw that there are all sorts of different models. Each of those models have different prior beliefs (as reflected by their prior distributions). Even though the Bayesian knowledge updating procedure is exactly the same for all models, having a different starting point means that they also have a different ending point. Specifically, for each model, their prior distribution is different, and their marginal likelihood is different (as indicated by the yellow bars in the Figures in the previous chapter).\nThe models shown so far differ in their “learning ability”. Generally, the uninformed model is a great learner: because it has a relatively weak starting point, it lets the data speak for itself. This is reflected by the posterior distribution being highly similar to the likelihood.5 In order to illustrate the differences in the models, and how they learn, we can go over how Sarah’s model and David’s model learned from the data.\n\n3.4.1 Sarah’s Learning Process\nWhile Alex was casting a very wide net, Sarah was doing the opposite. Rather than considering all values of \\(\\theta\\), Sarah was absolutely sure that there was only one possible value of \\(\\theta\\), namely \\(0.5\\).\n\n\n\n\n\nFigure 3.6: Sarah’s model for a coin toss. The arrow indicates that only a single value for theta is postulated.\n\n\n\n\nIn other words, Sarah’s prior belief about \\(\\theta\\) was quite fanatical. The prior density of all values other than 0.5 is 0, and the prior density at 0.5 is infinitely high (since infinity cannot be conveniently shown in a figure, we use the arrow to indicate infinity). So what happens when someone is absolutely convinced that \\(\\theta = 0.5\\), and then gets presented with data? Perhaps another look at the Bayesian updating formula can give some insight: \\[\\begin{align}\n\\underbrace{ p(\\theta \\mid \\text{data})}_{\\substack{\\text{Posterior beliefs}}} \\,\\,\\, = \\,\\,\\,\n\\underbrace{ p(\\theta)}_{\\substack{\\text{Prior beliefs} }}\n\\,\\,\\,\\, \\times\n\\overbrace{\\underbrace{\\frac{p( \\text{data} \\mid \\theta)}{p( \\text{data})}}}^{\\substack{\\text{Prediction for specific }\\theta }}_{\\substack{\\text{Average prediction} \\\\\\text{across all }  \\theta's}}.\n\\end{align}\\]\nSince Sarah’s prior density for any value that is not 0.5, is equal to 0, that means that the posterior density for those values will also be 0. Because of the multiplication by 0, the updating factor is therefore completely ignored. As for the pior density at the value 0.5 - it too gets mutliplied by the updating factor, but just as anything multiplied by 0 is 0, anything multiplied by infinity is infinity. In other words, a model like Sarah’s is completely blind to the data, since she is alread so convinced that \\(\\theta = 0.5\\).6 The posterior will therefore be exactly the same as in fig-sarah-model-binomial above.\nLet’s define the marginal likelihood once more: Sarah has prior beliefs about \\(\\theta\\), reflected by the prior mass in Figure 3.6. Each of these values has a certain match with the observed data (i.e., the likelihood). The marginal likelihood is then the average of all those likelihoods, weighted by the prior mass assigned. This weighting by prior mass makes each model’s marginal likelihood different from each other, because each has their own unique prior beliefs. If we were to look at the updating factor for Sarah, we would see that Sarah’s marginal likelihood is simply equal to the likelihood of the data for \\(\\theta = 0.5\\) (i.e, \\(P(\\text{data} \\mid \\theta = 0.5) =\\) 0.0439) because that is the only value that Sarah assigned any prior mass to.\n\n\n3.4.2 David’s Learning Process\nSomewhere in the middle of Alex’ ambivalence and Sarah’s fanaticism, there is David’s prior belief that only values between 0.5 and 1 are possible, and that values closer to 1 are more plausible than values closer to 0.5. As you can see in Figure 3.7 below, this means that values between 0 and 0.5 are assigned 0 prior density - a process known as truncation. Truncation makes a model one-sided, since only values to one side of 0.5 have been assigned prior density, while the other side is set to 0. From Sarah’s learning process we saw that this means that David’s posterior beliefs will also be 0 for values below 0.5. So what does David’s learning process look like?\n\n\n\n\n\nFigure 3.7: David’s model for a coin toss. David believes that only values between 0.5 and 1 are possible, and that values closer to 1 are more plausible, a priori. When certain ranges of values have their density set to 0, we refer to the distribution as a truncated distribution.\n\n\n\n\nJust as before, we start with the prior beliefs, update these with the updating factor (which values in the model predicted the data better/worse than average?), to form posterior beliefs. The difference between the different models lies in their different starting points (prior beliefs), but also their updating factor differs. Specifically, the likelihood stays the same (Figure 3.2), but the marginal likelihood differs. As we see in Figure 2.5, David’s marginal likelihood for the outcome of 8 heads is approximately 0.174.\nIn the previous chapter we saw how we can use the ratio of the marginal likelihoods of two models (i.e., the Bayes factor) to do model comparison. In this chapter we focus on individual models and how they estimate/learn about the parameter. In that context we use the marginal likelihood to see which parameter values in the model predicted the data better/worse than average, in order to see which values receive a boost/penalty in plausibility. The figure below illustrates this mechanism - note that the likelihood is exactly the same as for other models, but the marginal likelihood (indicated by the purple bar) is different.\n\n\n\n\n\nFigure 3.8: The likelihood of observing 8 heads out of 10 flips, for all possible values of theta. The purple line is the marginal likelihood of David’s model, to visualize which values predicted the observed data better than average.\n\n\n\n\nWhen we apply the Bayesian knowledge updating, we arrive at David’s posterior knowledge. As before, the values that predicted better than average will have a higher posterior density than prior density, while the reverse holds for values of \\(\\theta\\) that predicted the data worse than average. Note that David excluded some values of \\(\\theta\\) a priori (similar to Sarah), namely those values between 0 and \\(0.5\\). Since the prior density for those values is 0, the posterior density will also be 0, regardless of the observed data. For David’s posterior distribution, the median equals \\(0.801\\), and the 95% credible interval ranges from \\(0.568\\) to \\(0.95\\). Although this is fairly similar to Alex’ posterior statistics (median \\(=0.764\\), 95% CI = \\([0.482, 0.940]\\)), they are not identical because both started with different prior beliefs and so will have different posterior beliefs. The interpretation of the median and CI will be the same, but now conditional on David’s model: under David’s model, there is a 95% probability that \\(\\theta\\) is between 0.568 and 0.95.\n\n\n\n\n\nFigure 3.9: Prior and posterior distribution for David. Values that predicted the data better than average have received a boost in plausibility (i.e., posterior density &gt; prior density), whie the reverse holds for values that predicted the data poorly."
  },
  {
    "objectID": "03-estimation.html#prior-distribution-in-bayesian-parameter-estimation",
    "href": "03-estimation.html#prior-distribution-in-bayesian-parameter-estimation",
    "title": "3  How do Models Estimate?",
    "section": "3.5 Prior Distribution in Bayesian Parameter Estimation",
    "text": "3.5 Prior Distribution in Bayesian Parameter Estimation\nWe have now seen three different posterior distributions, for the three people (and their different prior beliefs) under consideration so far. As we have seen, the posterior distribution of Sarah is identical to her prior distribution, since she was so convinced about \\(\\theta = 0.5\\). The posterior distributions for David and Alex are pretty similar, although not identical. This reflects a general mechanism in Bayesian parameter estimation: the more uninformed a prior distribution is, the more it lets “the data speak for itself”. In other words, the more peaked (i.e., informed) a prior distribution is, the more data are needed to “overthrow” such a strong prior conviction. In Bayesian inference for a proportion, this mechanism is neatly reflected by how the prior and posterior distribution, as well as the data, are built up. To illustrate, we can take a closer look at how we usually specify a prior distribution for a proportion.\n\n3.5.1 The Beta Distribution\nIn Bayesian inference, the prior distribution can be any probability distribution (depending on which models you want to compare, or which model is most suitable for estimation). One requirement for the prior distribution is that it matches the domain of the parameter. In the case of a proportion, this means that the prior distribution ranges from 0 to 1. One such family of distributions is the beta distribution. The beta distribution can take on many different shapes, based on the value of its two shape parameters \\(\\alpha\\) and \\(\\beta\\) (sometimes also just written in Latin, a and b). Setting these shape parameters changes how the beta distribution looks. You can play around in this applet or this applet, to get an idea.\n\n\n\nBasically, these are the dynamics:\n\nSetting \\(a = b\\) creates a symetric distribution, with \\(a = b = 1\\) giving a uniform distribution.\n\nAs the value of \\(a = b\\) increases, more and more probability mass will be centered in the middle (at 0.5), with Sarah’s prior distribution as the limit (\\(a = b = \\infty\\)).\n\nWhen \\(a &gt; b\\), more probability mass will be to the right of 0.5, while the reverse holds for \\(a &lt; b\\).\nWhen \\(a &lt; 1\\) and \\(b &lt; 1\\), more mass will be towards the tails of the distribution (close to 0 and 1). Note that \\(a\\) and \\(b\\) need to be greater than 0.\n\n\n\n3.5.2 Beta Interpretation\nIn the context of a prior distribution for a proportion, the \\(a\\) and \\(b\\) can be interpreted as previously observed heads and tails. For instance, having a uniform prior distribution (like Alex) can be seen as having observed 1 heads and 1 tails already. David’s prior distribution is in fact a beta distribution with \\(a = 3\\) and \\(b = 1\\), truncated for lower values (meaning values \\(&lt; 0.5\\) receive 0 probability mass). This corresponds to David having already seen 3 heads and 1 tails. In order to obtain Sarah’s prior distribution, we would have a beta distribution with \\(a = b = \\infty\\), which means that Sarah believes as if she has already seen incredibly many heads and tails.\nIn addition to the prior distribution, the posterior distribution for \\(\\theta\\) is also a beta distribution. Specifically, it is a beta distribution where \\(a\\) is equal to the \\(a\\) of the prior distribution, plus the number of observed succeses/heads. The \\(b\\) is equal to the \\(b\\) of the prior distribution, plus the number of observed failures/tails. For instance, Alex’ posterior distribution (Figure 3.5) is a beta distribution with \\(a = 1 + 8 = 9\\), and \\(b = 1 + 2 = 3\\), while David’s posterior distribution (Figure 3.9) is a (trunctated) beta distribution with \\(a = 3 + 8 = 11\\), and \\(b = 1 + 2 = 3\\). Representing the knowledge updating in terms of the beta distribution also illustrates how Sarah’s prior and posterior distribution are the same: her posterior \\(a\\) is equal to \\(\\infty + 8 = \\infty\\), and her posterior \\(b\\) is equal to \\(\\infty + 2 = \\infty\\).\n\n\n3.5.3 Two-sided vs One-sided Estimation\nTypically, when applying Bayesian estimation, we use the most uninformed model, since this model has the least bias in it. In addition, a two-sided model is generally used because the one-sided model can give misleading estimates in case it predicts the wrong side. For instance, if the true value of \\(\\theta\\) equals 0.2, David will never assign any posterior mass to it, no matter the evidence. If we imagine observing 10 tails and 1 heads, David’s posterior distribution will look as follows:\n\n\n\n\n\nPrior and posterior distribution for David, now with 10 tails and 1 heads. This illustrates how a one-sided model can give misleading estimates: the posterior median and credible interval here will still favor values greater than 0.5, even though the data give quite some evidence for lower values.\n\n\n\n\nWhile we can still take a 95% credible interval from this posterior, it is clear that it gives a misleading estimate (the 95% CI here will be from 0.5 to 0.65) . Since David’s model only considers values between 0.5 and 1, the values that receive a boost in plausibility are those values as close as possible to the observed proportion (\\(1/11\\) = 0.091), so all posterior mass “piles up” at its truncation threshold. A two-sided model has a lot more flexibility and will simply follow along in the direction that the observed proportion is in (greater or smaller than 0.5).\n\n\n3.5.4 An Endless Loop\nWhat would happen if tomorrow we gather the same people again, and collect a new set of data? On this new day, they will have prior distributions equal to the posterior distributions of today, ready for a new round of knowledge updating. As more and more data accumulate (either spread over multiple days, or updated all at once), the starting prior distribution grows less influential. However, the stronger/informed the prior, the longer this process takes."
  },
  {
    "objectID": "03-estimation.html#relation-to-hypothesis-testing",
    "href": "03-estimation.html#relation-to-hypothesis-testing",
    "title": "3  How do Models Estimate?",
    "section": "3.6 Relation to Hypothesis Testing",
    "text": "3.6 Relation to Hypothesis Testing\nSo far we have been talking about models. In order to conduct a hypothesis test, we can take several of these models and compare their predictive performance through the Bayes factor (see Section 2.2 and Section 2.4. Typically, the model with extreme conviction about a “null value”7, which in this case would be Sarah’s model. Then, what we call the alternative hypothesis can be any other model that corresponds to the theory one wants to test: all models we have seen so far make some specific statement about plausible values of \\(\\theta\\). The Bayes factor simply tells us which model predicted the data the best: it is a relative metric that compares two models at a time.\n\n3.6.1 The Savage-Dickey Density Ratio\nWhen employing such a null/alternative hypothesis setup, there exists a convenient computational trick to obtain the Bayes factor of one alternative model over the null model. When we take the updating of the alternative model, for instance Alex’, we can take the prior density at the value of testing (\\(0.5\\)) and the posterior density at the value of testing. The ratio of these two values is equal to the Bayes factor comparing Alex and Sarah’s models. The prior density at 0.5 equals 1 (i.e. \\(P(\\theta = 0.5) = 1\\)) and the posterior density at 0.5 equals approximately 0.5 (i.e., (\\(P(\\theta = 0.5 \\mid \\text{data}) \\approx 0.5\\))), so their ratio is approximately \\(1 / 0.5 = 2\\), in favor of the alternative hypothesis (i.e., \\(\\text{BF}_{10} \\approx 2\\)). The Savage-Dickey density ratio also implies the following: when the prior density is higher than the posterior density at the value of testing, we will find evidence in favor of the alternative hypothesis (the magnitude of evidence depends on how much lower the prior density is). This makes sense: the value of testing is the only value that is considered by the null hypothesis, so if that value has suffered a decline in plausibility as a result of the data, then that means the null hypothesis as a whole did not do well. Figure 3.10) below shows Alex’ prior and posterior distribution, with the grey dots indicating the density ratio.\nFor a different point of testing, for instance \\(0.8\\), we can take the Savage-Dickey density ratio at that point. With a test value of 0.8, the null model becomes equal to Paul’s model (his prior distribution was concentrated on the value 0.8), so we are comparing Alex’ model to Paul’s model. In Figure 3.10, the density values are indicated by the purple dots. Here, the null model (i.e., Paul’s model) did very well, since \\(0.8\\) fits the observed data very well, as indicated by the posterior density being much higher than the prior density at that point (\\(\\text{BF}_{01} \\approx 3\\)).\n\n\n\n\n\nFigure 3.10: Prior and posterior distribution for Alex, with the grey/purple dots indicating the prior/posterior density values for two test values: 0.5 and 0.8. The ratio of the grey values is equal to the Bayes factor comparing Alex’ and Sarah’s models, while the ratio of the purple values is equal to the Bayes factor comparing Alex’ and Paul’s models. This ratio is known as the Savage-Dickey density value."
  },
  {
    "objectID": "03-estimation.html#concluding-thoughts",
    "href": "03-estimation.html#concluding-thoughts",
    "title": "3  How do Models Estimate?",
    "section": "3.7 Concluding Thoughts",
    "text": "3.7 Concluding Thoughts\nIn this chapter, we have seen how different models learn from the same data to form posterior beliefs. These posterior beliefs about the parameter can then be used to make an estimation (i.e., a probabilistic statement) of the parameter in the form of a \\(x\\%\\) Credible Interval, or posterior median.\nWe have seen several key ideas:\n\nPrior knowledge (i.e., what a model believes, before seeing the data) is characterized by a probability distribution: in the case of proportion \\(\\theta\\), we typically use the beta distribution\nPrior knowledge is then updated with the updating factor, which consists of the likelihood of the observed data, and the average likelihood for that model\nValues of \\(\\theta\\) that predicted the data well will receive a boost in plausibilty (posterior density &gt; prior density) and values that predicted the data poorly will receive a penalty in plausibilty (prior density &gt; posterior density)\nThe posterior distribution reflects our posterior knowledge (i.e., what a model believes, after seeing the data), which we use to make an estimate of the parameter\nStrong prior convictions will need more data to be overthrown."
  },
  {
    "objectID": "03-estimation.html#footnotes",
    "href": "03-estimation.html#footnotes",
    "title": "3  How do Models Estimate?",
    "section": "",
    "text": "How would you characterize your beliefs about the number of penguins in the world? What would your belief distribution look like?↩︎\nThe prior distribution is the offical term for the probability distribution that quantifies a model’s prior beliefs about a parameter.↩︎\nTechnical sidenote: integrating over the likelihood will give the marginal likelihood for Alex.↩︎\nNote the contrast to the sampling distribution, which is a distribution of the sample statistic \\(\\hat{p}\\), rather than the parameter.↩︎\nRemember that the likelihood is not a probability distribution. You can see Bayesian statistics as a “trick” to turn the likelihood into a probability distribution (i.e., the posterior distribution), so that you can make probabilistic statements about the parameter!↩︎\nI feel like there is a real-life analogue to be found here somewhere…↩︎\nThe null value is typically the value that implies no effect, such as 0.5 for a proportion, or 0 for a correlation or t-test.↩︎"
  },
  {
    "objectID": "04-beer.html#methods",
    "href": "04-beer.html#methods",
    "title": "4  The Beer Tasting",
    "section": "4.1 Methods",
    "text": "4.1 Methods\nOn a Friday afternoon, May 12th 2017, an informal beer tasting experiment took place at the Psychology Department of the University of Amsterdam. The experimental team consisted of three members: one to introduce the participants to the experiment and administer the test, one to pour the drinks, and one to process the data. Participants tasted two small cups filled with Weihenstephaner Hefeweissbier, one with alcohol and one without, and indicated which one contained alcohol. Participants were also asked to rate the confidence in their answer (measured on a scale from \\(1\\) to \\(100\\), with \\(1\\) being completely clueless and \\(100\\) being absolutely sure), and to rate the two beers in tastiness (measured on a scale from \\(1\\) to \\(100\\), with \\(1\\) being the worst beer ever and \\(100\\) being the best beer ever). The experiment was double-blind, such that the person administering the test and interacting with the participants did not know which of the two cups contained alcohol. For ease of reference, each cup was labeled with a random integer between \\(1\\) and \\(500\\), and each integer corresponded either to the alcoholic or non-alcoholic beer. A coin was flipped to decide which beer was tasted first. The setup was piloted with \\(9\\) participants; subsequently, we tested as many people as possible within an hour, and also recorded which of the two beers was tasted first. On average, testing took approximately 30 seconds per participant, yielding a total of \\(57\\) participants. Of the 57 participants, 42 (\\(73.7\\%\\)) correctly identified the beer that contained alcohol: in other words, there were \\(s = 42\\) successes and \\(f = 15\\) failures.1"
  },
  {
    "objectID": "04-beer.html#analysis-in-jasp",
    "href": "04-beer.html#analysis-in-jasp",
    "title": "4  The Beer Tasting",
    "section": "4.2 Analysis in JASP",
    "text": "4.2 Analysis in JASP\nIn order to analyze the collected data in JASP, the Bayesian binomial test can be used, which can be found under the menu labeled “Frequencies”. Several settings are available for the binomial test, allowing exploration of different analysis choices. Figure 4.1 presents a screenshot of the options panel in JASP. For this analysis, we specify a test value of \\(0.5\\) (i.e., chance performance), and \\(a = b = 1\\) for the prior distribution of \\(\\theta\\) under \\(\\mathcal{H}_1\\). Note that in a sensitivity or robustness analysis (covered in the next chapter), other values for \\(a\\) and \\(b\\) may be explored to assess their impact on the posterior distribution.\n\n\n\n\n\nFigure 4.1: Screenshot of the options for the Bayesian binomial test in JASP.\n\n\n\n\nThe null hypothesis here postulates that participants perform at chance level, which implies \\(\\theta = 0.5\\) since if participants cannot taste the difference, they will just be guessing. The alternative hypothesis postulates that this is not the case. There are several options for the alternative hypothesis in terms of \\(a\\), \\(b\\), and its directionality. For instance, in the case of two-sided hypothesis testing, the hypotheses can be specified as follows: \\[\\begin{equation*}\n\\mathcal{H}_0:  \\theta = 0.5\n\\end{equation*}\\] \\[\\begin{equation}\n\\mathcal{H}_1:  \\theta \\sim \\text{beta}(1, 1)\\text{.}\n\\label{eq:hypoSetupTwoSided}\n\\end{equation}\\] However, since we wish to test whether or not participants’ discriminating ability exceeds chance, we can specify the alternative hypothesis to allow only values of \\(\\theta\\) greater than \\(0.5\\) (note the `\\(+\\)’ in the subscript): \\[\\begin{equation}\n\\mathcal{H}_+:  \\theta \\sim \\text{beta}(1, 1) \\text{I}  (0.5, 1)\\text{,}\n\\label{eq:hypoSetup}\n\\end{equation}\\] where I indicates truncation of the beta distribution to the interval \\([0.5, 1]\\).\nFigure 4.2 illustrates the results of the binomial test. The left panel shows the prior and the posterior distribution of \\(\\theta\\) for the two-sided alternative hypothesis, along with the median and credible interval of the posterior distribution. The posterior median equals \\(0.731\\) and the \\(95\\%\\) credible interval ranges from \\(0.610\\) to \\(0.833\\), indicating a substantial deviation of \\(\\theta\\) from \\(0.5\\). For each value of \\(\\theta\\), the change from prior distribution to posterior distribution is quantified by predictive adequacy: for those values of \\(\\theta\\) that predict the data better than average, the posterior density exceeds the prior density. Additionally, the two-sided Bayes factor \\(\\text{BF}_{10} = 112\\), which means the data are 112 times more likely under \\(\\mathcal{H}_1\\) than under \\(\\mathcal{H}_0\\).\nThe right panel shows inference for the one-sided positive hypothesis (i.e., \\(\\mathcal{H}_+: \\theta \\geq 0.5\\)) compared to the null hypothesis: the resulting Bayes factor is \\(225.26\\) in favor of the alternative hypothesis, which means the data are \\(225\\) times more likely under \\(\\mathcal{H}_+\\) than under \\(\\mathcal{H}_0\\). Note that the posterior distribution itself has hardly changed, compared to the two-sided result: the posterior median still equals \\(0.731\\) and the \\(95\\%\\) credible interval ranges from \\(0.610\\) to \\(0.833\\). Because virtually all posterior mass was already to the right of \\(0.5\\) in the two-sided case, the posterior distribution was virtually unaffected by changing from \\(\\mathcal{H}_1\\) to \\(\\mathcal{H}_+\\). However, in the right panel, \\(\\mathcal{H}_+\\) only predicts values greater than \\(0.5\\), which is reflected in the prior distribution: all prior mass is now located in the interval \\((0.5\\text{, } 1)\\), and as a result, the prior mass in the interval \\((0.5\\text{, } 1)\\) has doubled. If the posterior remains constant, but the prior mass doubles, this means that the Bayes factor also doubles (perhaps best illustrated by the Savage-Dickey density ratio). In other words, a model that is predicting the data well and is focusing more on those values (e.g., \\(\\mathcal{H}_+\\)) has more evidence in its favor than a model that is predicting the data well, but spread its bets more (e.g., \\(\\mathcal{H}_1\\)). Such a built in reward for making a more specific prediction is an important mechanism in Bayesian model comparison, and is known as parsimony. Making a more specific prediction comes with a risk though: \\(\\mathcal{H}_-\\) predicted the data a lot worse than \\(\\mathcal{H}_0\\) because it bet on the wrong direction.\n\n\n\n\n\nFigure 4.2: Bayesian binomial test for theta. The probability wheel at the top illustrates the ratio of the evidence in favor of the two hypotheses. The two gray dots indicate the Savage-Dickey density ratio. The median and the 95 percent credible interval of the posterior distribution are shown in the top right corner. The left panel shows the two-sided test and the right panel shows the one-sided test."
  },
  {
    "objectID": "04-beer.html#continuous-updating",
    "href": "04-beer.html#continuous-updating",
    "title": "4  The Beer Tasting",
    "section": "4.3 Continuous Updating",
    "text": "4.3 Continuous Updating\nThe beer tasting experiment also highlights one of the main strengths of Bayesian inference: real-time monitoring of the incoming data. As the data accumulate, the analysis can be continuously updated to include the latest results. In other words, the results may be updated after every participant, or analyzed all at one, without affecting the resulting inference. To illustrate this, we can compute the posterior distribution for the first \\(9\\) participants of the experiment for which \\(s = 6\\) and \\(f = 3\\). Specifying the same beta prior distribution as before, namely a truncated beta distribution with shape parameters \\(a = b = 1\\), and combining this with the data, yields a truncated beta posterior distribution with shape parameters \\(a = 6 + 1 = 7\\) and \\(b = 3 + 1 = 4\\) (see also Section 3.5.2). The resulting posterior distribution is presented in the left panel of Figure 4.3. Now, we can take the remaining 48 participants and update our knowledge once more. Because we already have knowledge about the population’s rate parameter \\(\\theta\\), namely the results of the first 9 participants, we can incorporate this in the analysis through the prior distribution, following Lindley’s maxim “today’s posterior is tomorrow’s prior” (Lindley 1972).\nIn this case, we start with a truncated beta prior distribution with \\(a = 7\\) and \\(b = 4\\), and update this with the data of the remaining 48 participants. Out of the 48 participants, 36 were correct, and 12 were incorrect. Updating the prior distribution with this data yields a posterior distribution with shape parameters \\(a = 7 + 36 = 43\\) and \\(b = 4 + 12 = 16\\), which is exactly the same posterior distribution obtained when analyzing the full data set at once. This two-step procedure is illustrated in Figure 4.3. The left panel shows the original prior distribution (i.e., the truncated beta distribution with \\(a = 1, b = 1\\)) and the posterior distribution for the first \\(9\\) participants. The right panel shows the inference for the remaining \\(48\\) participants, while starting with the posterior-turned-prior distribution reflecting the knowledge about the first \\(9\\) participants (a truncated beta distribution with \\(a = 7, b = 4\\)).\n\n\n\n\n\nFigure 4.3: Sequential updating of the Bayesian binomial test. The left panel shows results from a one-sided Bayesian binomial test for the first \\(n = 9\\) participants (\\(s = 6\\), \\(f = 3\\)). The shape parameters of the truncated beta prior were set to \\(a = 1\\) and \\(b = 1\\). The right panel shows results from a one-sided binomial test for the remaining \\(48\\) participants. Here, the specified prior is the posterior distribution from the left panel: a truncated beta distribution with \\(a+s = 7\\) and \\(b+f = 4\\). The resulting posterior distribution is identical to the posterior distribution in Figure 4.2. In order to obtain the total Bayes factor in Figure 4.2 (i.e., 225), the component Bayes factors from the left and right panel can be multiplied: \\(1.01 * 223 = 225\\).\n\n\n\n\nWe can even look at the evolution of the Bayes factor, as the data come in. This development can be inspected by means of a sequential analysis. Figure 4.3 shows the evolution of the Bayes factor as more data are collected. Initially the evidence is inconclusive, but after about \\(30\\) participants the evidence increasingly supports \\(\\mathcal{H}_1\\). Being able to monitor the data as such can be very useful while planning or conducting an experiment. For instance, instead of needing to already commit to a certain sample size, a researcher can simply keep going untill a certain evidence threshold (for either hypothesis) is reached.2\n\n\n\n\n\nFigure 4.4: Sequential analysis, showing the evolution of the Bayes factor as \\(n\\), the number of observed participants, increases. After an initial period of relative inconclusiveness, the Bayes factor strongly favors \\(H_+\\)."
  },
  {
    "objectID": "04-beer.html#concluding-thoughts",
    "href": "04-beer.html#concluding-thoughts",
    "title": "4  The Beer Tasting",
    "section": "4.4 Concluding Thoughts",
    "text": "4.4 Concluding Thoughts\nIn this chapter we compared several models that made a statement about the beer tasting ability of people. The null model was absolutely sure that people cannot taste the difference between alcoholic and non-alcoholic beer, while the positive alternative model (\\(\\mathcal{H}_+\\)) stated that people’s ability ranges uniformly from 0.5 to 1. The Bayes factor comparing the predictions of \\(\\mathcal{H}_+\\) and \\(\\mathcal{H}_0\\) indicated the observed proportion, \\(0.73\\) (\\(n = 57\\)), to be 225 times more likely under \\(\\mathcal{H}_+\\) than under \\(\\mathcal{H}_0\\). This can be seen as very strong evidence (see Figure 2.3) in favor of the theory that people can taste the difference between the two beers. In addition, Bayesian knowledge updating was illustrated by analyzing the data sequentially, as opposed to all at once.\n\n\n\n\nLindley, D. V. 1972. Bayesian Statistics, a Review. Philadelphia (PA): SIAM."
  },
  {
    "objectID": "04-beer.html#footnotes",
    "href": "04-beer.html#footnotes",
    "title": "4  The Beer Tasting",
    "section": "",
    "text": "Three video recordings of the procedure are available at this OSF repository.↩︎\nThis is in contrast to using \\(p\\)-values, where the data can only be analyzed once, and any additional look at the data will inflate the type I error rate.↩︎"
  },
  {
    "objectID": "05-more-tests.html#sec-bayesian-ttest",
    "href": "05-more-tests.html#sec-bayesian-ttest",
    "title": "5  More Bayesian Analyses",
    "section": "5.1 The Bayesian T-Test",
    "text": "5.1 The Bayesian T-Test\nThe first question, “do people find alcoholic beer tastier?”, concerns the difference between means. Since each participant tasted the alcoholic and non-alcoholic beer, this was measured within subjects, and so a within subjects \\(t\\)-test is required. For Bayesian \\(t\\)-tests, the parameter of interest is denoted \\(\\delta\\) (“delta”). This parameter is a standardized difference between two means, and is formally known as “Cohen’s d”, a very common effect size in psychology.1 When doing inferential statistics, we can either estimate the magnitude of this effect size, conduct model comparison, or both. In the sections below, the Bayesian ingredients are described for the \\(t\\)-test.\n\n\n\n\n5.1.1 Prior Distribution\nThe prior distribution is always defined on the same domain as the parameter of interest. For the proportion, this was the convenient domain of [0, 1], and so allowed the use of the beta distribution, and the possibility to use the uniform distribution as the uninformed prior distribution. The domain of \\(\\delta\\) instead goes from \\(-\\infty\\) to \\(\\infty\\), so its prior distribution has to match that domain. For a null hypothesis this does not matter so much, since the null hypothesis generally posits a single value (e.g., 0, stating there is no difference between the groups). However, for the alternative hypothesis it becomes tricky now to have a uniform distribution on the whole domain of \\(\\delta\\). Since the domain is infinitely big, the density of a uniform distribution between \\(-\\infty\\) and \\(\\infty\\) would need to be infinitely small, which is not very practical. Instead, what is generally done is to apply a probability distribution that is spread out a little less (although still a lot more than a point null hypothesis). One such distribution is the Cauchy distribution, which is a \\(t\\) distribution with a single degree of freedom. The width of the Cauchy distribution is determined by the Cauchy scale parameter. Below, several examples are given:\n\n\n\n\n\nFigure 5.1: Three different Cauchy distributions. Each of these can be used as a model for the difference between two groups, and each of these has a (slightly) different theoretical implication.\n\n\n\n\nJust as before, these distributions can serve as a statement about the population parameter. Also just as before, each of these models make predictions about the world, and will have a certain quality of their prediction: how well did they predict the data? We can look at how well they did, and compare it to how well the null model (which went “all-in” on 0) predicted the data. Before we do that, we can first take a look at how these models will learn from the data: how is this prior knowledge updated to form posterior knowledge?\n\n\n5.1.2 Predictive Updating Factor\nThe Bayesian belief updating again follows the general form presented in Chapter 3. Again, we update the prior knowledge with information from the data. In the case of the beer tasting experiment, there was an observed effect size of \\(0.714\\) (for more descriptives see Figure 5.2 below).\n\n\n\n\n\nFigure 5.2: The descriptive statistics for the tastiness ratings for both the alcoholic and non-alcoholic beers. The observed mean for the alcoholic beer is higher than for the non-alcoholic beer, but how much evidence is there in favor of this difference?\n\n\n\n\nThe predictive updating factor quantifies how well each of the values in the model predicted the observed effect of \\(0.714\\) (as quantified by the likelihood), compared to how well the model did on average (as quantified by the marginal likelihood). Figure 5.3 below shows the likelihood of the observed data for various values of \\(\\delta\\). The purple bar indicates the marginal likelihood for the one-sided Cauchy model (scale = 0.707), to show which values in that model will receive a boost in plausibility. Remember that it is the likelihood function that is the same for any model, but that the marginal likelihood of that model will differ (based on its predictions).\n\n\n\n\n\nFigure 5.3: The likelihood of the observed data, for various values of delta. The higher the likelihood, the better that value predicted the data.\n\n\n\n\n\n\n5.1.3 Posterior Distribution & Bayes Factor\nValues in the model that predicted the data well, will see in increase in density when comparing prior to posterior distribution. Figure 5.3 shows that values between 0.5 and 1 will receive a boost in plausibility. Figure 5.4 below shows the JASP-results for the Bayesian \\(t\\)-test, using a one-sided alternative hypothesis to test the hypothesis that people like the alcoholic beer more than the non-alcoholic beer. The posterior distribution is fairly concentrated between \\(0.5\\) and \\(1\\), with a 95% credible interval from 0.398 to 0.978, so that is already some evidence that the tastiness ratings differ between the two beers. In addition, the Bayes factor comparing the predictions of the two hypotheses shows that the data are 22200 times more likely under the alternative hypothesis \\(\\mathcal{H}_{+}\\) than under \\(\\mathcal{H}_{0}\\).\n\n\n\n\n\nFigure 5.4: The results of the Bayesian paired samples t-test on the tastiness ratings. The bayes factor comparing the predictions of the one-sided, positive, alternative hypothesis to the null hypothesis is very strongly in favor of the alternative hypothesis: the data are 22200 times more likely under the alternative hypothesis than under the null hypothesis.\n\n\n\n\n\n5.1.3.1 Bayes Factor Robustness\nSpecifying the prior distribution is a fairly subjective endeavor in Bayesian analyses. For most analyses, there exist some guiding principles for choosing an uninformative prior distribution, but it is still worth investigaing how robust the obtained Bayes factor is to different prior specifications. Since the Bayes factor compares the predictions of two models, changing the prior distribution changes the model’s prediction and therefore also alters the Bayes factor. To analyze to what extent this happens, a robustness check can be conducted, where different prior specifications are explored.\nFor the \\(t\\)-test, where the Cauchy prior distribution is governed by a single shape parameter (its scale), a convenient plot can be constructed, where the Bayes factor is shown as a function of the shape parameter. Figure 5.5 shows such a plot. Here we can see that there is quite strong evidence in favor of \\(\\mathcal{H}_{+}\\) for almost all Cauchy prior widths in the graph (i.e., the line is relatively flat). Only for extreme values of the Cauchy scale parameter (around \\(0.05\\)), does the evidence in favor of \\(\\mathcal{H}_{+}\\) decrease towards 1. This is a logical consequence of changing the prior distribution: the prior distribution formalizes a model’s predictions, and if the prior distribution becomes extremely narrow, it starts resembling the null model. For a Cauchy width of, say, \\(0.01\\), the alternative and null model make very similar predictions, and so the Bayes factor will be around 1. In the context of a robustness check, we can ignore such extreme specifications. We generally aim to detect whether, for instance, the Bayes factor with a width of 1 qualitatively differs from the Bayes factor with a width of 0.5. If that is the case, then our result is perhaps not so reliable, and we would need more data to create a more robust result.\n\n\n\n\n\n\nFigure 5.5: A robustness analysis of the Bayesian t-test. Here we explore how much the Bayes factor changes, as a result of using a different value for the Cauchy scale parameter. Generally, the flatter the line, the more robust the Bayes factor is to different prior specifications."
  },
  {
    "objectID": "05-more-tests.html#the-bayesian-correlation",
    "href": "05-more-tests.html#the-bayesian-correlation",
    "title": "5  More Bayesian Analyses",
    "section": "5.2 The Bayesian Correlation",
    "text": "5.2 The Bayesian Correlation\nIn addition to testing whether there is a difference in tastiness ratings, we can also analyze whether there is an association between the ratings of the two beers: are people who rate the alcoholic beer as tasty, more inclined to also rate the non-alcoholic beer as tasty? In other words, are there people who just really like Weihenstephaner (and give both beers high scores), and people who do not (and give both beers low scores)?\nIn order to do so, we can conduct a Bayesian correlation analysis. We will again be using all the key ingredients from the previous chapters. We will start with some prior distribution, then update this with the information in the observed data, to form posterior knowledge about the population correlation \\(\\rho\\) (“rho”). Additionally, we can conduct a hypothesis test, where we compare a model that states no association between the ratings, and a model that states that there is some positive association.\nTo conduct a Bayesian correlation test in JASP, you can select (after loading the data) “Regression”, then “Bayesian correlation”. This presents the correlation analysis for several variables. To obtain more results, you can go to “Plot Individual Pairs”, where JASP allows a more thorough analysis of individual pairs of variables. See Figure 5.6 for a screenshot of the current analysis.\n\n\n\n\n\nFigure 5.6: The JASP user interface for the Bayesian correlation analysis. To enable more analysis options, the “Plot Individual Pairs” tab can be used.\n\n\n\n\n\n5.2.1 Prior Distribution\nThe domain of the correlation is \\([-1, 1]\\), so we a prior distribution that matches that domain. In this case, we can take the beta distribution from before, and stretch its domain to create the stretched beta distribution. While before, the values of a and b can be specified individually, for the stretched beta distribution we only set a single value for both a and b: the prior width. The width is the inverse of \\(a\\) and \\(b\\): a width equal to 0.5 means a stretched beta distribution with \\(a\\) and \\(b\\) equal to \\(1 / 0.5 = 2\\). A width equal to 1 means a stretched beta distribution with \\(a = b = 1\\). Figure 5.7 shows three versions of the stretched beta distribution - additionally showing that these distributions can also be one-sided (i.e., only considering negative or positive correlations).\n\n\n\n\n\nFigure 5.7: Three different stretched beta distributions. Each of these can be used as a model for the correlation, and each of these has a (slightly) different theoretical implication.\n\n\n\n\n\n\n5.2.2 Predictive Updating Factor\nThe observed correlation \\(r\\) is equal to \\(0.1034\\), and we can look at how likely this result is for various values of the population correlation \\(\\rho\\). Figure 5.8 shows the likelihood function. The likelihood of observing a correlation of \\(0.1\\) is highest when the population correlation is in fact equal to \\(0.1\\). As before, the likelihood illustrates which values of \\(\\rho\\) have a good match (i.e., a good prediction) with the observed data.\nTo see which values in the model predicted the data better than average, we can look at the marginal likelihood for that model. The purple bar in Figure 5.8 shows the marginal likelihood for the two-sided model (prior width = 1). We use the marginal likelihood to see which values of \\(\\rho\\) deserve a boost in plausibilty, and later we will compare marginal likelihoods of different models to obtain a Bayes factor.\n\n\n\n\n\nFigure 5.8: The likelihood of the observed data, for various values of rho. The higher the likelihood, the better that value predicted the data. The likelihood is the highest for the observed correlation (0.1).\n\n\n\n\n\n\n5.2.3 Posterior Distribution & Bayes Factor\nThe updated beliefs about \\(\\rho\\) are shown in Figure 5.9. In order to test whether there is an association, we can look at the Bayes factor. Here, we have found moderate evidence in favor of the null hypothesis: the data are 4.5 times more likely under the null model, compared to the alternative model. This highlights an important feature of Bayesian hypothesis testing: since we concretely quantify what both models predict, we can actually obtain evidence in favor of the null hypothesis. This means we can distinguish between absence of evidence and evidence of absence. The former means that there is just no evidence to conclude that there is an association, while the latter means that we have found evidence for the lack of an association. In terms of Bayes factors, absence of evidence occurs when we observe a Bayes factor close to 1 (no evidence either way), while evidence of absence occurs when we observe \\(\\text{BF}_{01} &gt; 1\\). The evidence in favor of the null also highlights the Savage-Dickey density ratio: \\(\\rho\\) being equal to 0 has become more plausible as a result of the data (its posterior density is greater than its prior density). This means that models that bet a lot of money on this value (such as the null model) will do very well in model comparisons.\nIn terms of parameter estimation, we can look at the posterior median and credible interval. The posterior median is quite close to 0, and the 95% credible interval ranges from \\(-0.158\\) to \\(0.3436\\): under the two-sided uniform model, there is a 95% probability that the true value of \\(\\rho\\) lies in that interval.\n\n\n\n\n\nFigure 5.9: The posterior distribution of rho, based on a two-sided uniform prior distribution. Under this model, there is a 95% probability that rho is between -0.155 and 0.331. There is moderate evidence in favor of the null hypothesis: the data are 4.5 times more likely under the null model, compared to the alternative model."
  },
  {
    "objectID": "05-more-tests.html#concluding-thoughts",
    "href": "05-more-tests.html#concluding-thoughts",
    "title": "5  More Bayesian Analyses",
    "section": "5.3 Concluding Thoughts",
    "text": "5.3 Concluding Thoughts\nIn this chapter, we have seen the Bayesian concepts from the previous chapters, but then applied to different research questions/parameters. Instead of models making statements about \\(\\theta\\), we looked at parameters that govern a difference between means (\\(\\delta\\)) or an association (\\(\\rho\\)). With a different parameter comes a different type of prior distribution, since the prior distribution matches the domain of the parameter. However, everything that follows is exactly the same as for the binomial analysis: the prior distribution is updated using the (marginal) likelihood to form posterior beliefs. We can compare marginal likelihoods of different models to obtain a Bayes factor. To investigate the robustness of the Bayes factor to the choice of prior distribution, a robustness check can be conducted.\nLastly, the Bayes factor helps to distinguish between “evidence of absence” and “absence of evidence”. This is quite informative, since they have two distinct meanings. Traditionally in psychology, journals are mostly interested in “non-null” effects, since these are deemed a lot sexier (and also because the p-value cannot easily distinguish between EoA and AoE). This results in some stress for empirical researchers: what if you spent 2 years of your PhD project collecting data, and you do not find your hypothesized effect and therefore cannot publish? Being able to quantify evidence in favor of the null hypothesis can hopefully create a scientific discourse that is more inclusive towards null-findings."
  },
  {
    "objectID": "05-more-tests.html#footnotes",
    "href": "05-more-tests.html#footnotes",
    "title": "5  More Bayesian Analyses",
    "section": "",
    "text": "There are many more effect sizes that can quantify a difference in means, but for simplicity’s sake we focus on Cohen’s d here.↩︎"
  },
  {
    "objectID": "06-exercises.html#binomial-test",
    "href": "06-exercises.html#binomial-test",
    "title": "6  Exercises",
    "section": "6.1 Binomial Test",
    "text": "6.1 Binomial Test\n\n6.1.1 Therapeutic Touch\nTherapeutic Touch (TT) is a nursing practice rooted in mysticism but alleged to have a scientific basis. Practitioners of TT claim to treat medical conditions by using their hands to manipulate a “human energy field” perceptible above the patients’ skin. Being a skeptical mind, 9-year old Emily Rosa ventured to test these claims by designing an experiment where TT practitioners had to correctly identify the location of the experimenter’s hand (above their left or right hand) while being blinded by a screen. The results of this experiment were later published in the prestigious Journal of the American Medical Association (Rosa et al., 1998). In the following exercise, you will evaluate this data set with JASP.\n\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nOpen the “Emily Rosa” dataset from the JASP Data Library (you can find it in the folder “Frequencies”). Alternatively, use the link above to view the results without using JASP.\nGet a descriptive overview of the data by producing a frequency table of the variable Outcome (Go to “Frequencies”, then “Bayesian Binomial Test” and select the variable “Outcome”). How many practitioners guessed correctly? What is the proportion?\nProduce a prior and posterior plot for a two-sided test and interpret the 95% credible interval displayed in it.\nConduct a Bayesian Binomial Test to test if the practitioners’ ability (i.e., proportion of correct responses) is better than random (i.e., \\(\\theta &gt; 0.5\\)). Use the default prior for the alternative hypothesis. What is the Bayes factor in favor of the null hypothesis (\\(\\text{BF}_{0+}\\))?\nHow would you interpret the Bayes factor?\nTake a look at the sequential analysis plot. What was (approximately) the largest Bayes factor in favor of the alternative hypothesis that was ever reached in the sequential analysis of the data?\nImagine a previous experiment with 10 practitioners, where 8 therapists gave the correct answer. Construct an informed prior distribution based on their results. How do the results of the hypothesis test change when compared to a default prior?\n\n\n\n6.1.2 Psychologists Tasting Beer\nFriday afternoon, May 12th 2017, an informal beer tasting experiment took place at the Department of Psychology, University of Amsterdam. The experiment was preregistered, the OSF directory with background information is https://osf.io/m3ju9/, and the corresponding article can be found on PsyArxiv.\nShort description: Each participant was (blinded) given two cups of Weihestephan Hefeweissbier, one cup with the regular variety, one cup with the non-alcoholic version. Participants had to indicate which cup contained the alcohol, the confidence in their judgment, and an assessment of how much they liked the beer. The data set contains the following variables:\n\nAlcBeerFist: 1 = alcoholic version was tasted first; 0 = alcoholic version was tasted second. This is not likely to matter, as participants were allowed to sample the cups at will.\nCorrectIdentify: 1 = correct, 0 = incorrect.\nConfidenceRating: Assessment of confidence in the identification judgment on a scale from 0 (complete guess) to 100 (certainty).\nAlcRating: Taste rating for the regular, alcoholic version, on a scale from 0 (worst beer ever) to 100 (best beer ever).\nNonAlcRating: Taste rating for the non-alcoholic version, on a scale from 0 (worst beer ever) to 100 (best beer ever).\n\nIn this exercise, you will conduct a Bayesian binomial test to analyze the proportion of correct responses.\n\ncsv file with the data\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nDownload the csv file from the OSF and open the data in JASP. Alternatively, use the link above to view the results without using JASP.\nGet a descriptive overview of the data by producing a frequency table of the variable Outcome (go to “Frequencies”, then “Bayesian Binomial Test” and select the variable “CorrectIdentify”). What percentage of participants had a correct response (“1”)?\nWhich settings can we use to specify our alternative hypothesis (see Figure @ref(fig:jasp-screenshot-binomial-test) for the available settings)? What do these settings imply?\nUsing a uniform prior distribution, what are the two-sided Bayes factors (\\(\\text{BF}_{10}\\)) for the proportion of incorrect responses (“0”) and for the proportion of correct responses (“1”)? Are they equal? Explain why (not).\nIn order to test whether people are able to taste the difference, conduct a one-sided hypothesis test to test if \\(\\theta &gt; 0.5\\). What is the Bayes factor (\\(\\text{BF}_{+0}\\))? How do you interpret this? What is your verdict on the claim that psychologists can taste the difference between alcoholic and non-alcoholic beer?\nConduct a Sequential analysis to inspect how the Bayes factor developed as the data accumulated. Did it ever provide strong evidence for the null hypothesis? Around which value did the Bayes factor cross the threshold of 100?\nImagine being someone who is very confident in psychologists beer tasting ability, because they once did their own experiment and saw that 10 out of 11 could taste the difference. What would their prior distribution look like? How does this affect the one-sided Bayes factor (\\(\\text{BF}_{+0}\\)) for the proportion of correct responses?"
  },
  {
    "objectID": "06-exercises.html#correlation",
    "href": "06-exercises.html#correlation",
    "title": "6  Exercises",
    "section": "6.2 Correlation",
    "text": "6.2 Correlation\n\n6.2.1 Correlation: A.W.E.S.O.M.-O 4000\nIn South Park episode 116, one of the series’ main protagonists, Eric Cartman, pretends to be a robot from Japan, the “A.W.E.S.O.M.-O 4000”. When kidnapped by Hollywood movie producers and put under pressure to generate profitable movie concepts, Cartman manages to generate thousands of silly ideas, 800 of which feature Adam Sandler. We conjecture that the makers of South Park believe that Adam Sandler movies are profitable regardless of their quality. In this exercise, we put forward the following South Park hypothesis: “For Adam Sandler movies, there is no correlation between box-office success and movie quality (i.e.,”freshness” ratings on Rotten Tomatoes; www.rottentomatoes.com).”\n\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nOpen the “Adam Sandler” dataset from the JASP Data Library (you can find it in the folder “Regression”).\nProduce a boxplot for the variables “Freshness” and ``Box Office ($M)“. Are there any outliers in the data?\nConduct a Bayesian Correlation Pairs test to test if there is any correlation is absent or present (\\(\\rho = 0\\) vs. \\(\\rho\\neq 0\\)). Use the default prior (``Stretched beta prior width” \\(= 1\\)) for the alternative hypothesis. What is the correlation coefficient \\(r\\)?\nProduce a prior and posterior plot and interpret the median and 95% credible interval displayed in it.\n\nHow much did the posterior probability at \\(\\rho = 0\\) increase with regards to the prior probability? How are these points displayed in JASP? And how is this reflected in the result of the analysis?\nWhat is the Bayes factor in favor of the null hypothesis (\\(\\text{BF}_{01}\\))? How would you interpret the Bayes factor?\nNow perform a one-sided test to test whether the correlation is positive (\\(\\rho = 0\\) vs. \\(\\rho&gt; 0\\)). How does this Bayes factor (\\(\\text{BF}_{+0}\\)) differ from the two-sided Bayes factor(\\(\\text{BF}_{10}\\)) – is it higher or lower?\nProduce a Bayes Factor Robustness Plot. What is the effect of the prior width on the Bayes factor (\\(\\text{BF}_{0+}\\))? What does this mean for our result?\nAdam Sandler doesn’t approve of our hypothesis, comes out and says that movie critics on Rotten Tomatoes purposely down-rate his awesome movies. He argues that there should be a negative correlation between box-office success and the quality of his movies. Perform a Bayesian Correlation Test with Adam’s alternative hypothesis. Would you support him in his claim? Why (not)?"
  },
  {
    "objectID": "06-exercises.html#t-test",
    "href": "06-exercises.html#t-test",
    "title": "6  Exercises",
    "section": "6.3 T-Test",
    "text": "6.3 T-Test\n\n6.3.1 The Effect of Directed Reading Exercises\nA teacher believes that directed reading activities in the classroom can improve the reading ability of elementary school children. She convinces her colleagues to give her the chance to try out the new method on a random sample of 21 third-graders. After they participated for 8 weeks in the program, the children take the Degree of Reading Power test (DRP). Their scores are compared to a control group of 23 children who took the test on the same day and followed the same curriculum apart from the reading activities. In the following exercise, you will evaluate this dataset with JASP.\n\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nOpen the “Directed Reading Activities” dataset from the JASP Data Library (you can find it in the folder “T-Tests”).\nGet a descriptive overview of the data.\n\nCreate a table that shows the means and standard deviations of DRP scores in the control and treatment group (specify “drp” as the “Variable” and specify “group” as the “Split”).\nCreate a boxplot that shows the distribution of DRP scores for each group.\n\nUsing the Bayesian independent samples \\(t\\)-test (use the variable “group” as the “Grouping Variable”), with the default prior distribution, obtain a posterior distribution for the standardized effect size \\(\\delta\\). What is the 95% credible interval? Is 0 in this interval? What if you used a 99% credible interval?\nHow would you interpret the credible interval?\nNow conduct a Bayesian independent samples \\(t\\)-test to test whether the control group performs worse than the treatment group (i.e., the mean of the control group is lower than the mean of the treatment group). Use the default prior for the alternative hypothesis. What is the Bayes factor in favor of the alternative hypothesis (\\(\\text{BF}_{-0}\\))?\nHow would you interpret the Bayes factor?\nCreate a Bayes factor robustness plot.\n\nWhat is the maximum Bayes factor in favor of the alternative hypothesis that you could achieve by tinkering with the scale parameter of the Cauchy distribution?\nWhich scale parameter corresponds to the maximum Bayes factor?\nIs the default prior that you used wider or narrower than the prior corresponding to the maximum Bayes factor?\nWould you say that the Bayes factor is robust against different formulations of the prior?\n\n\n\n\n6.3.2 Psychologists Tasting Beer 2: T-Test Boogaloo\nFriday afternoon, May 12th 2017, an informal beer tasting experiment took place at the Department of Psychology, University of Amsterdam. The experiment was preregistered, the OSF directory with background information is https://osf.io/m3ju9/, and the corresponding article can be found at https://psyarxiv.com/d8bvn/.\nShort description: Each participant was (blinded) given two cups of Weihestephan Hefeweissbier, one cup with the regular variety, one cup with the non-alcoholic version. Participants had to indicate which cup contained the alcohol, the confidence in their judgment, and an assessment of how much they liked the beer. The data set contains the following variables:\n\nAlcBeerFist: 1 = alcoholic version was tasted first; 0 = alcoholic version was tasted second. This is not likely to matter, as participants were allowed to sample the cups at will.\nCorrectIdentify: 1 = correct, 0 = incorrect.\nConfidenceRating: Assessment of confidence in the identification judgment on a scale from 0 (complete guess) to 100 (certainty).\nAlcRating: Taste rating for the regular, alcoholic version, on a scale from 0 (worst beer ever) to 100 (best beer ever).\nNonAlcRating: Taste rating for the non-alcoholic version, on a scale from 0 (worst beer ever) to 100 (best beer ever).\n\nIn this exercise, we revisit this data set but instead apply a series of \\(t\\)-tests in order to answer two questions. First, was there a meaningful difference in confidence between people that were correct and people that were incorrect? Second, did people like the alcoholic beer better than the non-alcoholic beer?\n\ncsv file with the data\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nWhat sort of \\(t\\)-test (e.g., independent samples or paired samples) do we need for the first research question? And for the second research question?\nDownload the csv file from the OSF and open the data in JASP.\nConduct a Bayesian independent samples \\(t\\)-test on the confidence ratings, and group by “CorrectIdentify”. Tick the boxes to get a table with the descriptive statistics, and a raincloud plot (displayed horizontally or vertically). What can you learn from these statistics?\nWhat is the one-sided Bayes factor (\\(\\text{BF}_{-0}\\)), where the alternative hypothesis postulates that confidence ratings are higher for the correct responses than for the incorrect responses?\nWhat can we conclude about the confidence ratings?\nConduct a paired samples \\(t\\)-test in order to test whether participants preferred the taste of the alcoholic beer over the taste of the non-alcoholic beer. To do so, go to “Bayesian Paired Samples T-Test” and then drag the two variables with the ratings (“AlcRating” and “NonAlcRating”) to the “Variable Pairs” box.\nWhat is Bayes factor comparing the one-sided alternative hypothesis (which postulates that alcoholic beer is tastier than non-alcoholic beer) to the null hypothesis?\nTick the box “Descriptives” under “Additional Statistics” to get a table with some descriptive and inferential statistics for each of the two variables. What is the difference between the two means?\nWhat can you conclude about the taste of the two beers?"
  },
  {
    "objectID": "06-exercises.html#summary-stats",
    "href": "06-exercises.html#summary-stats",
    "title": "6  Exercises",
    "section": "6.4 Summary Stats",
    "text": "6.4 Summary Stats\n\n6.4.1 T-Test: Flag Priming\nIn this exercise, we conduct a reanalysis of the article “A Single Exposure to the American Flag Shifts Support Toward Republicanism up to 8 Months Later” (Carter, Ferguson, and Hassin (2011)):\n\n… tested whether a single exposure to the American flag would lead participants to shift their attitudes, beliefs, and behavior in the politically conservative direction. We conducted a multisession study during the 2008 U.S. presidential election.\n\nThe study featured various outcome measures and analyses, but for the purposes of this exercise we focus on the following result:\n\nAs predicted, participants in the flag-prime condition (\\(M = 0.072, SD = 0.47\\)) reported a greater intention to vote for McCain than did participants in the control condition \\((M = -0.070, SD = 0.48), t(181) = 2.02, p = .04, d = 0.298)\\).\n\nWe assume that the total sample size was 183, and that there were 92 people in the flag-prime condition and 91 in the control condition. How strong is the evidence that corresponds to this \\(p=.04\\) result?\n\nJASP output with only the results\nJASP output with results and interpretation/solutions\nJASP file with results and interpretation/solutions\n\n\nUse the Summary Stats module in JASP to conduct a two-sided Bayesian independent samples \\(t\\)-test. Interpret the strength of evidence. Additionally, produce a plot of the prior and posterior distribution for the standardized effect size.\nCompare the Bayesian result from the previous question to the frequentist p-value. What is the p-value here? Does this p-value lead to a different conclusion? Assume an \\(\\alpha\\) level of 0.05.\nSince the authors’ hypothesis is clearly directional, conduct a one-sided Bayesian independent samples \\(t\\)-test (where the alternative hypothesis states that the flag group scored greater than the control group). Interpret the strength of evidence for the authors’ claim.\nConduct a Bayes factor robustness check analysis to investigate the strength of evidence for the authors’ claim across a number of different prior settings. What is the most evidence we could have in favor of the one-sided alternative hypothesis?\nDiscuss your results with your classmate/neighbor/cat – do they agree?\n\n\n\n\n\nCarter, T. J., M. J. Ferguson, and R. R. Hassin. 2011. “A Single Exposure to the American Flag Shifts Support Toward Republicanism up to 8 Months Later.” Psychological Science 22: 1011–18."
  },
  {
    "objectID": "07-anova.html#more-beer-analyses",
    "href": "07-anova.html#more-beer-analyses",
    "title": "7  ANOVA (draft)",
    "section": "7.1 More Beer Analyses",
    "text": "7.1 More Beer Analyses\nContinuing with the beer example, we can expand on the regression equation above, and add another regression weight that encodes whether someone correctly identified the beer:\n\\[ tastiness = b_0 + b_1 \\times alcoholic + b_2 \\times correct. \\] Here, the variables alcoholic and correct are both indicator variables (taking on values 0 or 1), to indicate whether someone was rating the alcoholic beer or not, and whether they were correct or not. These indicators thus determine if we add \\(b_1\\) or \\(b_2\\) to our estimate for the tastiness rating. With another variable added, we now have various models that we can make:\n\n\\(\\mathcal{M_0}\\): model with only the intercept \\(b_0\\)\n\\(\\mathcal{M_A}\\): model with the intercept \\(b_0\\) and the main effect of alcohol \\(b_1\\)\n\\(\\mathcal{M_C}\\): model with the intercept \\(b_0\\) and the main effect of correct identification \\(b_2\\)\n\\(\\mathcal{M}_{A+C}\\): model with the intercept \\(b_0\\) and the two main effects\n\nEach of these models makes various statements about all the possible regression weights. Again, what can help is think of the betting analogy: the model is at the casino, and bets money on likely values of each of the parameters. Some models go “all-in” on some values (e.g., Sarah and Paul from Section 2.1, who did not spread bets across multiple values, but put all money on 0). For instance, \\(\\mathcal{M_A}\\) will spread their bets across multiple values for \\(b_1\\), but will strictly bet on 0 for \\(b_2\\). In doing so, \\(\\mathcal{M_A}\\) allows for difference in tastiness ratings between alcoholic and non-alcoholic beer, but is not modeling any effect of whether the beer was correctly identified. Exactly how \\(\\mathcal{M_A}\\) spreads their bets on the values for \\(b_1\\) depends on how their prior distribution is specified.\n\n7.1.1 Priors in ANOVA\nIn Bayesian inference, the prior distribution of a model characterizes what a model predicts. Typically, a null model will have a spike prior on 0, while an alternative model will allow the parameter to vary. Exactly how the model allows the parameter to vary is indicated by its prior distribution. For the \\(t\\)-test, the prior distribution is specified for the effect size \\(\\delta\\), which is generally done by the Cauchy distribution (Section 5.1.1 discusses the Cauchy prior more elaborately). This distribution has a single scale parameter that can be adjusted, to make the model bet an a wider or narrower range of values. By default, the scale parameter is set to \\(\\frac{1}{\\sqrt{2}} = 0.707\\), which means that the alternative model bets around 50% of its money on values between -0.707 and 0.707. The reasoning behind this is that these are conventional effect sizes found in social sciences.\nIn expanding to ANOVA (or, more than 1 effect, or more than 2 groups), we are adding additional \\(b\\) parameters to the model. Each of these parameters requires a prior distribution, to make concrete what each model is predicting. The generalization for the Cauchy distribution is here to instead use a multivariate Cauchy distribution for each factor variable, which is also governed by a single scale parameter.2\nThe reasoning with the prior distributions in the ANOVA is exactly the same as for the \\(t\\)-test: the prior reflects which values of the parameter are being bet on. Based on the observed data, some of those values predicted the data well, and some other will have predicted the data poorly. We can look at how well the model predicted overall, by looking at how well it predicted the data on average: averaged over all the values that it bet on. The average quality of a model’s predictions is its marginal likelihood.\nThe figures below illustrate how each of the three “alternative” models \\(\\mathcal{M_A}\\), \\(\\mathcal{M_C}\\), \\(\\mathcal{M_{A+C}}\\) can formalize their predictions through their prior distributions. When a model posits there is no effect of a predictor, such as \\(\\mathcal{M_A}\\) for \\(b_2\\), they go “all-in” on the value and have a spike prior at 0.\n\n\n\n\n\nFigure 7.1: The prior distributions for the model with only the main effect of the beer being alcoholic or not.\n\n\n\n\n\n\n\n\n\nThe prior distributions for the model with only the main effect of whether the alcoholic beer was correctly identified.\n\n\n\n\n\n\n\n\n\nThe prior distributions for the model with both main effects.\n\n\n\n\nOnce again, different models predict different things about the data, and their predictions are made concrete by their prior distributions. With the predictions of each model formalized, we can take a look at the observed data, and see how well each model predicted that data!\n\n\n7.1.2 Results\nFor each model, we can compute the marginal likelihood: how well did all values postulated by the model (as reflected by the prior distributions) match the data, on average? Now, in the earlier chapters it was still somewhat comprehensive to show these calculations. As our models grow more complex however, the computations that underlie the analyses become extremely complex (the specifics are also out of my own grasp), and we need modern computers with magical algorithms to compute the marginal likelihoods of each model. Thankfully, in 2023 everyone possesses a fast enough computer (ideally with JASP installed on it), so conducting a Bayesian ANOVA can be done with the press of a button.3 Figure 7.2 below shows how the interface looks for specifying the analysis in JASP. Since we have a mixed design, we use the RM ANOVA, where both within and between subject factors can be specified (note: we omit the interaction effect for now).\n\n\n\n\n\nFigure 7.2: The interface of the Bayesian RM ANOVA in JASP. The difference in alcoholic and non-alcoholic beer was measured within each person, and whether the alcoholic beer is correctly identified was measured between person.\n\n\n\n\n\n\n7.1.3 Model Comparison\nWith JASP having done the computational lifting, we can either do parameter estimation through a credible interval, or model comparison/hypothesis testing through the Bayes factor. For starters, we can do model comparison by looking at the first table that JASP outputs in the RM ANOVA:\n\n\n\n\n\nFigure 7.3: The model comparison table for the main effects models.\n\n\n\n\nThere are quite some numbers in there so let’s unpack these one by one:\n\nP(M): The prior model probability - how likely is each model, before seeing the data? Typically we evenly divide the odds between all models. Here, we have four models, so assign 0.25 to each model.\n\nInterpretation for BeerType model: “Before looking at the data there is a 25% probability that this model is the true model, out of these four models”.\n\nP(M|data): The posterior model probability - how likely is each model, after seeing the data? For this column, all values also sum to 1.\n\nInterpretation for BeerType model: “After looking at the data there is a 44.6% probability that this model is the true model, out of these four models”.\n\n\\(\\textbf{BF}_{M}\\): The posterior model odds - the updating factor from prior to posterior model probability.\n\nFor the BeerType model, we compute this by taking the prior and posterior odds: \\(0.446/(1 - 0.446) \\times (1 - 0.25)/0.25 \\approx 2.42\\).\nInterpretation for BeerType model: “The data are 2.42 times more likely under this model, than under all the other models combined”\n\n\\(\\textbf{BF}_{10}\\): The pairwise Bayes factor - how likely are the data under this model, compared to another model? By default, JASP gives the comparison between the model in the row, and the best model (i.e., with the highest posterior model probability). For instance, to compute \\(\\text{BF}_{10}\\) for the “BeerType” model, we take the ratio of its posterior model probability, and divide it by the posterior model probability of the best model (i.e., “BeerType + CorrectIdentify”): \\(0.446 / 0.554 \\approx 0.81\\).\n\nInterpretation for BeerType model: “The data are 0.81 times more likely under this model, than under the best model”.\n\nThe error percentage is an estimate of the numerical accuracy of the underlying algorithms. See the section below for a more elaborate explanation.\n\nThere are many different numbers to look at here, for each model. Personally, I would say the two Bayes factor columns are the most informative: these allow direct comparisons between the models. Based on which column (\\(\\text{BF}_{M}\\) or \\(\\text{BF}_{10}\\)), there are different comparisons. While \\(\\text{BF}_{10}\\) shows the pairwise comparisons (which is the Bayes factor we have seen before in this booklet), \\(\\text{BF}_{M}\\) shows a comparison between a single model, and all other models combined. Based on either of the BF columns, we can see that the model with the two main effects predicted the data the best (so in in the first row, \\(\\textbf{BF}_{10}\\) compares the two main effects model to itself), although the model with only BeerType is not doing so much worse: the data are 0.81 times more likely under this model, than under the model with the two main effects.\nBecause \\(\\textbf{BF}_{10}\\) is a ratio between two models’ marginal likelihoods, we can use the Bayes factors in that column to conduct additional model comparisons. For instance, to compare the BeerType model directly to the null model, we can take those two models’ \\(\\textbf{BF}_{10}\\) values and divide them by each other: \\(0.806 / 8.284e-6 \\approx 97296\\). Therefore, the data are 97296 times more likely under the BeerType model than under the null model. This property is known as Bayes factor transitivity.\nSo far, this procedure is fairly similar to the frequentist ANOVA, in the sense that we are comparing different model to each other (although here we are using way more intuitive and useful model comparison metrics…). However, the Bayesian ANOVA also allows us to look at the combined evidence in favor of the inclusion of a certain predictor. To do so, we tick the “effects” checkbox in JASP.\n\n\n7.1.4 Analysis of Effects\nInstead of looking at individual models, and how well they predicted the data, we can instead look at each predictor variable, and how well the models predicted that included that predictor. In doing so, we are again comparing groups of models, instead of comparing a single model to a single model. For instance, if we want to quantify the evidence we have for including BeerType as a predictor variable, we can look at how well the models predicted that included it, versus the models that did not include it. By doing so, we are comparing the models [BeerType; BeerType + CorrectIdentify] to the models [null; CorrectIdentify].\nFor this comparison, we can look at the metrics presented in the Analysis of Effects table:\n\n\n\n\n\nFigure 7.4: The effects table for the two predictor variables.\n\n\n\n\nAgain there are quite some numbers to unpack. Below they are defined, including an example, and how they relate to the model comparison table.\n\nP(incl): The prior inclusion probability. Here we sum all the prior model probabilities of the models that include this predictor.\n\nCorrectIdentify is included in 2 models that each have a prior model probability of 0.25, so its prior inclusion probability is 0.5.\n\nP(incl|data): The posterior inclusion probability. Here we sum all the posterior model probabilities of the models that include this predictor.\n\nCorrectIdentify is included in 2 models that have a posterior model probability of 0.563 and something very small, so its posterior inclusion probability is approximately 0.563.\n\n\\(\\textbf{BF}_{incl}\\) : The inclusion Bayes factor. It quantifies the change from prior inclusion probability to posterior inclusion probability for each component.\n\nFor CorrectIdentify, this is calculated as follows: \\(0.563/(1 - 0.563) * (1 - 0.5)/0.5 \\approx 1.29\\)\n\n\nThe most important column here is the inclusion Bayes factor: it quantifies how well the models with a certain predictor do, compared to the models without. We can see here that there is overwhelming evidence in favor of including BeerType, but that there is not so much evidence for including CorrectIdentify (also no evidence in favor of its exclusion - this is known as absence of evidence, rather than evidence of absence).\n\n\n7.1.5 Single Model Inference\nTo dive deeper into what a single model predicts, we can look at the “Single Model Inference” option in JASP. Here, we can specify a specific model, and then get estimates of its various parameters (i.e., its \\(b\\)’s). Below is a table for the estimates of the model with the two main effects:\n\n\n\n\n\nFigure 7.5: The estimates for the model with the two predictor variables.\n\n\n\n\nWhat the table presents here, are the mean estimates (and credible intervals) for the group differences. It is encoded a bit differently, such that \\(0.5b\\) is added to/subtracted from the alcoholic and non-alcoholic condition. Still, it characterizes what this specific model predicts for certain participants in certain situations. For instance, if we had to give our best guess of a person tasting a non-alcoholic beer, who correctly identified it, based on the model with two main effects? \\[48.4 + (-9.637) + (-4.151)  = 34.552\\] And if they would have guess incorrectly: \\[48.4 + (-9.637) + (4.151) =  42.854\\]\n\n\n7.1.6 Model Averaging\nInstead of looking at a single model’s predictions, we can combine all the models into a single prediction. Since each model makes different predictions, we can combine their predictions by model averaging. This method weighs each model’s prediction by their posterior model probability, which ensures that the models that predicted the best, will also influence the model averaged predictions the most. A very silly cartoon that illustrates this concept can be found here. The model averaged estimates are shown below. Since they are mostly dictated by the model with two main effects (the best model), these estimates do not differ so much from the previous estimates. In the same way, we can request visual representations of these estimates by checking the option for “model averaged posteriors” (the plots might need some resizing). Alternatively, individual model’s posterior distributions can be requested in the “Single Model Inference” tab.\n\n\n\n\n\nFigure 7.6: The estimates for the model with the two predictor variables.\n\n\n\n\n\n7.1.6.1 Error percentage\nIn order to obtain the marginal likelihood and posterior distributions in the Bayesian ANOVA (and many Bayesian analyses), a type of method called Markov chain Monte Carlo (MCMC) sampling is used. Since these methods are sampling-based, it introduces slight variations in the results. For instance, running the same Bayesian ANOVA twice will lead to slight fluctuations in the Bayes factors. In JASP, the degree of fluctuation is indicated by the “error %” column. These percentages indicate how much the Bayes factor will fluctuate from run to run. Generally, having an error % below 20 (or 10, if you want to be more strict) is acceptable: it means that a Bayes factor of 10 will deviate between 8 and 12, which does not change its qualitative interpretation. See also this footnote in the tutorial paper."
  },
  {
    "objectID": "07-anova.html#even-more-beer-analyses---interaction-effect",
    "href": "07-anova.html#even-more-beer-analyses---interaction-effect",
    "title": "7  ANOVA (draft)",
    "section": "7.2 Even More Beer Analyses - Interaction Effect?!",
    "text": "7.2 Even More Beer Analyses - Interaction Effect?!\nAs practice, try adding an interaction effect between BeerType and CorrectIdentify. What do you conclude?\n\n\n\n\nvan den Bergh, Don, Johnny Van Doorn, Maarten Marsman, Tim Draws, Erik-Jan Van Kesteren, Koen Derks, Fabian Dablander, et al. 2020. “A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP.” L’Année Psychologique/Topics in Cognitive Psychology. 120: 73–96."
  },
  {
    "objectID": "07-anova.html#footnotes",
    "href": "07-anova.html#footnotes",
    "title": "7  ANOVA (draft)",
    "section": "",
    "text": "For an extra primer on how to write ANOVA as a regression, please read here↩︎\nDue to some underlying computational reasons, the scale is specified in a different way in the Bayesian ANOVA in JASP, compared to the \\(t\\)-test. To get perfect agreement between the t-test and the ANOVA, the scale parameter for the ANOVA should be set to \\(\\frac{s}{\\sqrt{2}}\\), where \\(s\\) is the scale used for the \\(t\\)-test. For example, if the prior scale is set to 0.707 for the \\(t\\)-test, the BF will be the same for the \\(t\\)-test and ANOVA if we conduct an ANOVA with its scale set to \\(\\frac{0.707}{\\sqrt{2}} = 0.5\\).↩︎\nAlthough transitioning more to this “black box” framework is not so satisfying as being able to show all the computations.↩︎"
  }
]